name: Auto-Scraper JDIH

on:
  schedule:
    - cron: '0 1 * * *'
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 lxml
    
    - name: Run scraper
      run: |
        python scraper.py
    
    - name: Commit and push if changed
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add regulations.json
        git diff --quiet && git diff --staged --quiet || (git commit -m "Auto-update regulations" && git push)
    
    - name: Success notification
      if: success()
      run: |
        echo "Scraping completed successfully"
    
    - name: Error notification
      if: failure()
      run: |
        echo "Scraping failed"
```

4. **Scroll ke bawah**
5. **Klik "Commit changes..."**
6. **Klik "Commit changes"** lagi

---

### **STEP 2: Verify Python File Exists**

Python code yang tadi ada di `scraper.yml` seharusnya ada di file terpisah bernama `scraper.py`.

1. **Klik "Code"** tab (balik ke root repository)
2. **Check apakah ada file `scraper.py`?**
   - âœ… Jika **ADA**: Good! Lanjut ke Step 3
   - âŒ Jika **TIDAK ADA**: Harus create dulu

**Jika tidak ada, create `scraper.py`:**

1. Klik "Add file" â†’ "Create new file"
2. Nama file: `scraper.py`
3. Copy-paste isi dari artifact **"scraper.py - JDIH Auto Scraper"** yang saya buatkan sebelumnya
4. Commit

---

### **STEP 3: Test Workflow**

1. **Klik tab "Actions"**
2. **Refresh** (F5)
3. Sekarang harus muncul **"Auto-Scraper JDIH"** di sidebar kiri
4. **Klik "Auto-Scraper JDIH"**
5. **Klik "Run workflow"** dropdown
6. **Klik "Run workflow"** button hijau
7. **Tunggu 1-2 menit**, refresh
8. Harus muncul workflow run dengan status

---

## **ğŸ“‹ STRUKTUR FILE YANG BENAR:**

Pastikan struktur seperti ini:
```
peraturan-esdm-klhk/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ scraper.yml         â† YAML config (GitHub Actions)
â”œâ”€â”€ scraper.py                  â† Python code (scraper logic)
â”œâ”€â”€ requirements.txt            â† Python dependencies
â”œâ”€â”€ regulations.json            â† Data storage
â”œâ”€â”€ index.html                  â† Website
â””â”€â”€ README.md                   â† Documentation
