# .github/workflows/scraper_v3.yml
name: Scraper V3 - ESDM & KLHK
on:
  workflow_dispatch:
    inputs:
      keywords_esdm:
        description: 'ESDM Keywords (comma-separated)'
        required: false
        default: 'ESDM,energi,mineral,batubara,migas'
        type: string
      keywords_klhk:
        description: 'KLHK Keywords (comma-separated)'
        required: false
        default: 'KLHK,lingkungan,kehutanan,konservasi'
        type: string
      keywords_perda:
        description: 'Perda Keywords (comma-separated)'
        required: false
        default: 'Perda,peraturan daerah'
        type: string
      limit_per_run:
        description: 'Maximum items per scraping run'
        required: false
        default: '50'
        type: string
  schedule:
    # Runs every day at 2 AM UTC (9 AM WIB)
    - cron: '0 2 * * *'

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4 lxml

      - name: Load config from repo (for scheduled runs)
        id: load_config
        run: |
          if [ -f "config.json" ]; then
            echo "Config file found, loading..."
            echo "CONFIG_EXISTS=true" >> $GITHUB_OUTPUT
          else
            echo "No config file, using defaults"
            echo "CONFIG_EXISTS=false" >> $GITHUB_OUTPUT
          fi

      - name: Run scraper with custom config
        run: |
          # Use workflow inputs if provided (manual trigger), otherwise use defaults/config.json
          KEYWORDS_ESDM="${{ github.event.inputs.keywords_esdm || 'ESDM,energi,mineral,batubara,migas' }}"
          KEYWORDS_KLHK="${{ github.event.inputs.keywords_klhk || 'KLHK,lingkungan,kehutanan,konservasi' }}"
          KEYWORDS_PERDA="${{ github.event.inputs.keywords_perda || 'Perda,peraturan daerah' }}"
          LIMIT="${{ github.event.inputs.limit_per_run || '50' }}"
          
          echo "=== Scraper Configuration ==="
          echo "Keywords ESDM: $KEYWORDS_ESDM"
          echo "Keywords KLHK: $KEYWORDS_KLHK"
          echo "Keywords Perda: $KEYWORDS_PERDA"
          echo "Limit: $LIMIT"
          echo "============================"
          
          # Run scraper with parameters
          # Modify your scraper script to accept these arguments
          python .scripts/scraper_v3.py \
            --keywords-esdm "$KEYWORDS_ESDM" \
            --keywords-klhk "$KEYWORDS_KLHK" \
            --keywords-perda "$KEYWORDS_PERDA" \
            --limit "$LIMIT"

      - name: Commit and push changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A
          
          # Check if there are changes
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            TIMESTAMP=$(date -u +'%Y-%m-%d %H:%M:%S UTC')
            git commit -m "Auto-update V3 - $TIMESTAMP"
            git push
            echo "Changes committed and pushed"
          fi

---

# .github/workflows/scraper_perda_v4.yml  
name: Scraper Perda V4
on:
  workflow_dispatch:
    inputs:
      keywords_perda:
        description: 'Perda Keywords (comma-separated)'
        required: false
        default: 'Perda,peraturan daerah,peraturan bupati,peraturan walikota'
        type: string
      limit_per_run:
        description: 'Maximum items per scraping run'
        required: false
        default: '50'
        type: string
  schedule:
    # Runs every Sunday at 2 AM UTC (9 AM WIB)
    - cron: '0 2 * * 0'

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4 lxml

      - name: Load config from repo (for scheduled runs)
        id: load_config
        run: |
          if [ -f "config.json" ]; then
            echo "Config file found, loading..."
            echo "CONFIG_EXISTS=true" >> $GITHUB_OUTPUT
          else
            echo "No config file, using defaults"
            echo "CONFIG_EXISTS=false" >> $GITHUB_OUTPUT
          fi

      - name: Run scraper with custom config
        run: |
          # Use workflow inputs if provided (manual trigger), otherwise use defaults/config.json
          KEYWORDS_PERDA="${{ github.event.inputs.keywords_perda || 'Perda,peraturan daerah,peraturan bupati,peraturan walikota' }}"
          LIMIT="${{ github.event.inputs.limit_per_run || '50' }}"
          
          echo "=== Scraper Configuration ==="
          echo "Keywords Perda: $KEYWORDS_PERDA"
          echo "Limit: $LIMIT"
          echo "============================"
          
          # Run scraper with parameters
          python .scripts/scraper_perda_v4.py \
            --keywords "$KEYWORDS_PERDA" \
            --limit "$LIMIT"

      - name: Commit and push changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A
          
          # Check if there are changes
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            TIMESTAMP=$(date -u +'%Y-%m-%d %H:%M:%S UTC')
            git commit -m "Auto-update Perda V4 - $TIMESTAMP"
            git push
            echo "Changes committed and pushed"
          fi
